{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "face_recognition.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paninilover04924/paninilover04924/blob/main/face_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QPj94nu6zrE"
      },
      "source": [
        "###Dependencies\n",
        "\n",
        "First we will import the dependencies, or libraries, necessary for this project. The dependencies are OpenCV for the face detector, skimage for reading an image from a url, and cv2_imshow for displaying the image in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvO5PwWjmXDl"
      },
      "source": [
        "#import dependencies\n",
        "import cv2\n",
        "from skimage import io\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqXJ71-JXYw3",
        "outputId": "63fbf9a3-8832-49f4-ff8e-726313645415"
      },
      "source": [
        "cv2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'cv2.cv2' from '/usr/local/lib/python3.7/dist-packages/cv2/cv2.cpython-37m-x86_64-linux-gnu.so'>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VqccN6i7k3c"
      },
      "source": [
        "###Getting the Image and Classifier\n",
        "\n",
        "Here we will get the face detector from the OpenCV library and store it in the faceCascade library. We then put the image address/url of the image. The example url below is an image of a group of four people. If you want, you can also upload a picture of yourself to google colab and put the filepath instead of the url to run facial detection on images from your own computer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AeOvLI76slQ"
      },
      "source": [
        "#get the cascade classifier from the cv2 filepath\n",
        "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "#url of the image\n",
        "url = \"/content/Screen Shot 2021-03-13 at 8.29.16 AM.png\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Go2j6oIxZgwP",
        "outputId": "30cf74c1-6a22-4894-fcd7-23d67948b17c"
      },
      "source": [
        "url"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Screen Shot 2021-03-13 at 8.29.16 AM.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXRqLJQ58el8"
      },
      "source": [
        "###Preprocessing the Image\n",
        "\n",
        "We first use the io.imread() function to get the image from the url. However, we need to convert it to OpenCV format because we are using the OpenCV classifier. The io image comes in RGB color format and we convert it to BGR as that is the default for opencv. This is the image we will use to display the boxes around the faces. We also need to convert the image to grayscale as the OpenCV model only works on grayscale faces. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvtpyzf48e_r"
      },
      "source": [
        "#read the picture from the url and turn it to BGR format\n",
        "picture = cv2.cvtColor(io.imread(url), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "#convert picture to grayscale\n",
        "gray = cv2.cvtColor(picture, cv2.COLOR_BGR2GRAY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdgtO4LHcDWe"
      },
      "source": [
        "picture.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o0FhEKicZsn"
      },
      "source": [
        "type(picture)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy81TtfJcRry"
      },
      "source": [
        "gray.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT_4MvwJdQal"
      },
      "source": [
        "gray.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwqJV1AldlR4"
      },
      "source": [
        "gray.min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LlCanUydug2"
      },
      "source": [
        "2**8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcClRtog9Fpf"
      },
      "source": [
        "###Detect the Faces\n",
        "We execute the detectMultiScale method to detect the faces. \n",
        "\n",
        "* The first input is the **grayscale image** we want to use for detection. \n",
        "\n",
        "* **scaleFactor** is used if the image is too large and we make it smaller by a factor of 1.1 as the face detector can only detect faces in a certain range of sizes.\n",
        " \n",
        "* **minNeighbors** is one of the most important parameters in the model. Remember, the image is first split into many small sections before classification. If minNeighbors is 5, there must be 5 other parts, or sections, of a face around a certain section if that section can be classified as part of a face (because usually one part of the face is surrounded by other parts). If you make minNeighbors larger, than the model will be much more sure about the faces it detects but it might miss some faces. If you make this smaller, the model will detect more faces but it will also make more mistakes. \n",
        "\n",
        "* **minSize** is the minimum size a face must be in order for it to be viable for detection.  \n",
        "\n",
        "The method will output the coordinates of all the faces found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOH1m1Z69J9h"
      },
      "source": [
        "# Detect faces in the image\n",
        "faces = faceCascade.detectMultiScale(\n",
        "    gray,\n",
        "    scaleFactor=1.1,\n",
        "    minNeighbors=5,\n",
        "    minSize=(30, 30),\n",
        "    flags = cv2.CASCADE_SCALE_IMAGE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6MqDa0P9MVz"
      },
      "source": [
        "###Display the Results\n",
        "\n",
        "The display will first output the length of the list of faces, or the number of faces, to the user. Then, for each of the face coordinates, cv2 will draw a green rectangle around the faces. Finally cv2_imshow function will display the main image with the rectangles drawn on top."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHncZ2Obd79C"
      },
      "source": [
        "faces"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK59KmMWeL7M"
      },
      "source": [
        "\n",
        "for face in faces:\n",
        "    print(face)\n",
        "    print(\"this is the end of a loop\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx2rUAOH9N6p"
      },
      "source": [
        "#print the number of faces found\n",
        "print(f\"Found {len(faces)} faces!\")\n",
        "\n",
        "# Draw a rectangle around the faces\n",
        "for (x, y, w, h) in faces:\n",
        "    cv2.rectangle(picture, (x, y), (x+w, y+h), (0, 0, 255), 3)\n",
        "\n",
        "#show the image with the rectangle drawn around it\n",
        "cv2_imshow(picture)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}